# ComfyUI-SambaNova
The SambaNova API Node gives ComfyUI users the second fastest token output LLM's have to offer with context \
![image](https://github.com/user-attachments/assets/d3e7edbc-dec7-4c0e-9578-16a29a671126)\
The Nova APIv1 comes with a chat or completion type of chat. \
Chat is like all other chat bots where completion will take the users prompt and system prompt if used to make a mock chat between the LLM and user. \
This has not been used much myself so I dont know the perks of using completion chat. \
Nova also has a streaming mode feauture I have not used much if at all other than testing. \ 
Gpt will help if you dont want to read the API docs. \ 
https://community.sambanova.ai/c/welcome/4 \ 
This is the speed of tokens chat for the top models. \
![image](https://github.com/user-attachments/assets/9af8233b-b385-4676-92d5-9674afb63ae6)\
## CONS & PROS
Cons\
\
\
\
Pros\
\
\
\

## Dependencies
'' py lib
## Installation
IF using Windows Port version\
ComfyUI Folder\
``````\
Inside Custom_nodes Folder\
``````\
IF using Matrix \
Inside venv\Scripts\
```activate```\
```pip install ```\
Inside Custom_nodes Folder\
```git clone ```\
ComfyUI-Manager (WIP)
